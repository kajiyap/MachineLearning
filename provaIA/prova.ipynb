{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "789edbab",
   "metadata": {},
   "source": [
    "# Sistema RAG: PLN com Embedding em VectorDB\n",
    "\n",
    "Este notebook implementa um sistema de Retrieval-Augmented Generation (RAG) que permite fazer perguntas sobre documentos PDF.\n",
    "\n",
    "**Fluxo completo:** PDF → Ler → PLN → Chunks → Embedding → VectorDB → Pesquisa → Contexto"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47d97669",
   "metadata": {},
   "source": [
    "**Bibliotecas utilizadas:**\n",
    "- **LangChain**: Framework para dividir textos em chunks (pedaços) menores e gerenciáveis\n",
    "- **SentenceTransformers**: Converte texto em vetores numéricos (embeddings) para busca semântica\n",
    "- **pdfplumber**: Extrai texto de arquivos PDF mantendo a formatação\n",
    "- **spaCy**: Biblioteca avançada de PLN para tokenização, lematização e análise linguística\n",
    "- **NLTK**: Fornece stopwords (palavras irrelevantes) em português\n",
    "- **re**: Expressões regulares para limpeza de texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "dc88b38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\pedro\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Para chunks e embeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter # pip install langchain\n",
    "from sentence_transformers import SentenceTransformer # pip install sentence_transformers\n",
    "# Para leitura de PDF\n",
    "import pdfplumber # pip install pdfplumber\n",
    "# Para tratamento de texto\n",
    "import re\n",
    "import spacy # python -m spacy download pt_core_news_sm\n",
    "import nltk # pip install nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2966ea3",
   "metadata": {},
   "source": [
    "## Bibliotecas para Banco de Dados Vetorial (Vector Database)\n",
    "\n",
    "**O que é um VectorDB?**\n",
    "Um banco de dados vetorial armazena representações numéricas (embeddings) de textos e permite buscar documentos similares usando distância vetorial ao invés de correspondência exata de palavras.\n",
    "\n",
    "**Por que usar ChromaDB?**\n",
    "- **Busca semântica**: Encontra documentos por significado, não apenas por palavras-chave\n",
    "- **Rápido**: Otimizado para buscar entre milhões de vetores\n",
    "- **Simples**: Fácil de usar e não requer configuração complexa\n",
    "- **Em memória**: Perfeito para protótipos e aplicações pequenas/médias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "7accd94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bibliotecas para Banco de Dados Vetorial (Vector Database)\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4cc4fbb",
   "metadata": {},
   "source": [
    "## ETAPA 1: Leitura e Extração de Texto do PDF\n",
    "\n",
    "**Objetivo:** Extrair todo o conteúdo textual do arquivo PDF para processamento.\n",
    "\n",
    "**Como funciona:**\n",
    "1. O `pdfplumber` abre o arquivo PDF\n",
    "2. Percorre todas as páginas extraindo o texto\n",
    "3. Remove quebras de linha para criar um texto contínuo\n",
    "\n",
    "**Por que fazer isso?**\n",
    "- PDFs armazenam texto de forma complexa (posicionamento, fontes, etc.)\n",
    "- Precisamos de texto puro para aplicar PLN\n",
    "- Quebras de linha podem interferir no processamento posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b39d00a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASSO 1: LEITURA DO PDF\n",
    "# Função para extrair texto de todas as páginas do PDF\n",
    "\n",
    "def ler_pdf(caminho_pdf):\n",
    "    \"\"\"Extrai texto de todas as páginas de um arquivo PDF\"\"\"\n",
    "    leitor_pdf = pdfplumber.open(caminho_pdf)\n",
    "    print(f'Quantidade de páginas: {len(leitor_pdf.pages)}')\n",
    "    \n",
    "    texto = \"\"\n",
    "    total_linhas = 0\n",
    "    \n",
    "    # Percorre todas as páginas do PDF\n",
    "    for pagina in range(len(leitor_pdf.pages)):\n",
    "        texto_pagina = leitor_pdf.pages[pagina].extract_text()\n",
    "        # Conta linhas na página (separadas por \\n)\n",
    "        linhas_pagina = texto_pagina.count('\\n') + 1\n",
    "        total_linhas += linhas_pagina\n",
    "        texto += texto_pagina\n",
    "    \n",
    "    print(f'Total de linhas no PDF: {total_linhas}')\n",
    "\n",
    "    # Remove quebras de linha para facilitar o processamento\n",
    "    texto = texto.replace(\"\\n\", \" \")\n",
    "    return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "eb5b7fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantidade de páginas: 1\n",
      "Total de linhas no PDF: 30\n",
      "PDF carregado: 2219 caracteres\n",
      "Preview: BUSCA INFORMADA Diferentemente da Busca Exaustiva, onde não se sabe qual o melhor nó de fronteira a ser expandido, a Busca Heurística estima qual o melhor nó da fronteira a ser expandido com base em f...\n"
     ]
    }
   ],
   "source": [
    "# APLICANDO PASSO 1: Carregamento e exibição do conteúdo do PDF\n",
    "arquivo_pdf = 'C:\\\\Development\\\\provaIA\\\\dataset\\\\buscainformada.pdf'\n",
    "texto_pdf = ler_pdf(arquivo_pdf)\n",
    "\n",
    "# Verificando o resultado da extração\n",
    "print(f\"PDF carregado: {len(texto_pdf)} caracteres\")\n",
    "print(f\"Preview: {texto_pdf[:200]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81c04bb7",
   "metadata": {},
   "source": [
    "## ETAPA 2: Configuração do PLN e Stopwords\n",
    "\n",
    "**Justificativa Técnica Profunda:**\n",
    "O PLN é crucial para **normalizar** o texto antes de gerar embeddings. Sem PLN, palavras como \"correndo\", \"correr\", \"corre\" gerariam embeddings diferentes, mesmo tendo o mesmo conceito. A lematização garante que todas sejam reduzidas a \"correr\", melhorando a **consistência semântica** e reduzindo a **dimensionalidade do vocabulário**.\n",
    "\n",
    "Além disso, stopwords (\"a\", \"de\", \"para\") ocupam uma boa parte do texto mas contribuem quase nada para o significado. Removê-las:\n",
    "- Reduz ruído nos embeddings\n",
    "- Acelera o processamento\n",
    "- Foca embeddings em palavras-chave relevantes\n",
    "\n",
    "**O que são Stopwords?**\n",
    "Palavras muito comuns que não agregam significado semântico (\"a\", \"de\", \"para\", \"com\", etc.). \n",
    "\n",
    "**Por que usar spaCy?**\n",
    "- Modelo treinado especificamente para português (pt_core_news_sm)\n",
    "- Realiza múltiplas tarefas em um único processamento (tokenização + lematização + POS tagging)\n",
    "- Mais preciso que ferramentas simples (entende contexto linguístico)\n",
    "- Lemmatização baseada em morfologia, não em regras heurísticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "fd512e16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Stopwords carregadas: 209 palavras\n",
      "✓ Stopwords carregadas: {'nossos', 'temos', 'hajamos', 'do', 'isso', 'tem', 'esta', 'só', 'tuas', 'nos', 'haver', 'era', 'estava', 'isto', 'há', 'aqueles', 'tivéssemos', 'pela', 'aquele', 'tiveram', 'aos', 'estar', 'esteja', 'houverá', 'estivermos', 'minha', 'não', 'esse', 'tiverem', 'tém', 'houvera', 'numa', 'essas', 'u', 'dos', 'seria', 'tenho', 'houvessem', 'fora', 'com', 'for', 'num', 'em', 'pelo', 'hão', 'os', 'fôramos', 'estes', 'ser', 'dele', 'seus', 'serei', 'vos', 'teríamos', 'entre', 'estamos', 'fosse', 'seja', 'de', 'seremos', 'ao', 'que', 'teus', 'i', 'nossas', 'tu', 'vocês', 'meus', 'meu', 'sejam', 'e', 'mais', 'estivéssemos', 'estivera', 'estejamos', 'fui', 'houvermos', 'para', 'houveram', 'hajam', 'aquilo', 'quem', 'fôssemos', 'por', 'serão', 'deles', 'suas', 'houverei', 'sua', 'me', 'éramos', 'forem', 'houvéssemos', 'nossa', 'depois', 'te', 'teriam', 'uma', 'nosso', 'mesmo', 'houvéramos', 'esteve', 'delas', 'está', 'muito', 'teremos', 'é', 'teve', 'aquela', 'das', 'um', 'estivesse', 'terá', 'tínhamos', 'tivermos', 'houve', 'estou', 'também', 'eles', 'tinha', 'houverão', 'tivéramos', 'houverem', 'tinham', 'somos', 'será', 'eram', 'estive', 'fossem', 'esses', 'houveríamos', 'hei', 'essa', 'já', 'estiveram', 'as', 'nas', 'terei', 'haja', 'sem', 'havemos', 'você', 'qual', 'sejamos', 'nós', 'estiver', 'foi', 'seríamos', 'estávamos', 'seu', 'até', 'foram', 'às', 'elas', 'nem', 'pelos', 'estas', 'tenha', 'estiverem', 'estivemos', 'da', 'lhes', 'estão', 'seriam', 'teria', 'tivera', 'ele', 'eu', 'houveria', 'tenham', 'se', 'aquelas', 'formos', 'minhas', 'fomos', 'este', 'tivemos', 'no', 'tua', 'tivesse', 'estivessem', 'estejam', 'houvesse', 'sou', 'mas', 'estivéramos', 'tive', 'houver', 'são', 'estavam', 'a', 'dela', 'ela', 'pelas', 'terão', 'o', 'na', 'à', 'como', 'houvemos', 'lhe', 'quando', 'tiver', 'tenhamos', 'houveriam', 'ou', 'teu', 'tivessem', 'houveremos'} palavras\n"
     ]
    }
   ],
   "source": [
    "# PASSO 2: CONFIGURAÇÃO DO PLN (Processamento de Linguagem Natural)\n",
    "\n",
    "# Carregar o modelo de linguagem portuguesa do spaCy\n",
    "nlp = spacy.load(\"pt_core_news_sm\")\n",
    "\n",
    "# Definir palavras irrelevantes (stopwords) que serão removidas\n",
    "api_stop_words = set(stopwords.words('portuguese'))  # Stopwords padrão do NLTK\n",
    "minhas_stop_words = {'a','e','i','o', 'u'}  # Vogais adicionais\n",
    "stop_words = api_stop_words | minhas_stop_words  # União dos conjuntos\n",
    "\n",
    "print(f\"✓ Stopwords carregadas: {len(stop_words)} palavras\")\n",
    "print(f\"✓ Stopwords carregadas: {(stop_words)} palavras\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "527276ea",
   "metadata": {},
   "source": [
    "### Função de Tratamento PLN: Lematização\n",
    "\n",
    "**O que faz esta função?**\n",
    "Aplica 5 técnicas de PLN para limpar e normalizar o texto:\n",
    "\n",
    "1. **Normalização**: Converte tudo para minúsculas (\"Casa\" = \"casa\")\n",
    "2. **Remoção de ruído**: Elimina números, pontuação e caracteres especiais\n",
    "3. **Tokenização**: Divide o texto em palavras individuais (tokens)\n",
    "4. **Remoção de stopwords**: Elimina palavras irrelevantes\n",
    "5. **Lematização**: Reduz palavras à sua forma canônica (lema)\n",
    "\n",
    "**IMPORTANTE: Lematização vs Stemming**\n",
    "\n",
    "**Lematização (usado aqui):**\n",
    "- Reduz palavras ao seu **lema** (forma base no dicionário)\n",
    "- Exemplos: \"correndo\" → \"correr\", \"melhores\" → \"bom\", \"crianças\" → \"criança\"\n",
    "- **Preserva o significado** da palavra\n",
    "- Mais lento, mas muito mais preciso\n",
    "\n",
    "**Stemming (NÃO usado):**\n",
    "- Apenas corta o final das palavras\n",
    "- Exemplos: \"correndo\" → \"corr\", \"melhores\" → \"melhor\"\n",
    "- Pode **confundir o sentido**: \"estudante\" e \"estudar\" viram \"estud\"\n",
    "- Mais rápido, mas menos preciso\n",
    "\n",
    "**Por que Lematização é melhor para RAG?**\n",
    "Como vamos usar embeddings semânticos, precisamos que as palavras mantenham seu significado real. O lema é o \"cerne\" da palavra, sua essência semântica, enquanto o stemming pode criar ambiguidade e prejudicar a busca."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "5a4ba43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para fazer o tratamento de linguagem natural usando spaCy\n",
    "# IMPORTANTE: Usa LEMATIZAÇÃO (não stemming) para preservar o significado das palavras\n",
    "def tratamento_pln(texto):\n",
    "\n",
    "    # 1. Normalização: Colocar o texto em minúsculas\n",
    "    # JUSTIFICATIVA: Garante que \"Machine\", \"machine\" e \"MACHINE\" sejam tratadas como a mesma palavra\n",
    "    # Sem isso, geraríamos 3 embeddings diferentes para o mesmo conceito, desperdiçando espaço vetorial\n",
    "    # e reduzindo a precisão da busca (a pergunta \"machine learning\" não encontraria \"Machine Learning\")\n",
    "    texto = texto.lower()\n",
    "\n",
    "    # 2. Remoção de números, pontuações e caracteres especiais\n",
    "    texto = re.sub(r'[^a-zA-Záéíóú\\s]', '', texto) # na expressão regular estão as exceções\n",
    "\n",
    "    # 3. Tokenização com spaCy\n",
    "    doc = nlp(texto)\n",
    "    tokens = [token.text for token in doc]\n",
    "\n",
    "    # 4. Remoção de stopwords, remoção de pontuação\n",
    "    # e Lematização (clean_tokens = tokens lematizados e sem stopwords)\n",
    "    clean_tokens = [token.lemma_ for token in doc if token.text not in\n",
    "    stop_words and not token.is_punct]\n",
    "\n",
    "    # 5. Juntar tokens lematizados de volta em uma string\n",
    "    clean_text = ' '.join(clean_tokens)\n",
    "\n",
    "    return clean_text\n",
    "    #return texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "f3cc14ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ PLN aplicado: 2219 → 1586 caracteres (redução de 633)\n",
      "Preview tratado: busco informar diferentemente busca exaustivo onde saber bom nó fronteira expander busca heurístico estimo bom nó fronteira expander base fune heuríst...\n"
     ]
    }
   ],
   "source": [
    "# APLICANDO PASSO 2: Processamento de Linguagem Natural no texto\n",
    "texto_pdf_tratado = tratamento_pln(texto_pdf)\n",
    "\n",
    "# Comparando tamanhos antes e depois do processamento\n",
    "reducao = len(texto_pdf) - len(texto_pdf_tratado)\n",
    "print(f\"✓ PLN aplicado: {len(texto_pdf)} → {len(texto_pdf_tratado)} caracteres (redução de {reducao})\")\n",
    "print(f\"Preview tratado: {texto_pdf_tratado[:150]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c107f65",
   "metadata": {},
   "source": [
    "## ETAPA 3: Criação de Chunks (Fragmentação)\n",
    "\n",
    "**O que são Chunks?**\n",
    "Pedaços pequenos de texto que podem ser processados e buscados independentemente.\n",
    "\n",
    "**Justificativa Técnica Profunda:**\n",
    "O chunking evita que o modelo de embeddings receba um texto que ultrapasse seu limite máximo de tokens (geralmente 256-512 tokens), e também **melhora a granularidade na recuperação semântica**:\n",
    "\n",
    "- **Chunk grande demais** → Gera embeddings muito genéricos que capturam múltiplos tópicos, reduzindo a precisão da busca\n",
    "- **Chunk pequeno demais** → Perde contexto suficiente para entender o significado completo, gerando fragmentos sem sentido\n",
    "- **Chunk balanceado (150 chars)** → Mantém uma ideia completa sem misturar tópicos diferentes\n",
    "\n",
    "**Por que dividir o texto em chunks?**\n",
    "1. **Limitação de tamanho**: Modelos de embedding têm limite de tokens (~256-512 tokens)\n",
    "2. **Precisão**: Chunks menores = busca mais precisa (um chunk = um conceito)\n",
    "3. **Contexto**: Cada chunk mantém contexto local suficiente sem ruído\n",
    "4. **Performance**: Busca vetorial é mais eficiente com vetores específicos\n",
    "\n",
    "**Parâmetros importantes:**\n",
    "- `chunk_size=150`: Tamanho máximo de cada chunk (em caracteres)\n",
    "\n",
    "- `chunk_overlap=30`: Sobreposição entre chunks para não perder contexto nas bordasNote a sobreposição \"é incrível. Usa\" que mantém o contexto.\n",
    "\n",
    "  - **Por que overlap?** Evita que uma frase importante seja \"cortada ao meio\" entre dois chunks\n",
    "\n",
    "- Chunk 2: \"é incrível. Usa algoritmos para aprender.\"\n",
    "\n",
    "**Exemplo:**- Chunk 1: \"O machine learning é incrível. Usa\"\n",
    "Texto: \"O machine learning é incrível. Usa algoritmos para aprender.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "bd5f4a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Texto dividido em 14 chunks\n",
      "Exemplo do primeiro chunk: busco informar diferentemente busca exaustivo onde saber bom nó fronteira expander busca heurístico ...\n"
     ]
    }
   ],
   "source": [
    "# PASSO 3: CRIAÇÃO DE CHUNKS (Divisão do texto em pedaços menores)\n",
    "# Dividir o texto em chunks para facilitar a busca semântica\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=150,      # Tamanho máximo de cada chunk\n",
    "    chunk_overlap=30     # Sobreposição entre chunks para manter contexto\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(texto_pdf_tratado)\n",
    "\n",
    "print(f\"✓ Texto dividido em {len(chunks)} chunks\")\n",
    "print(f\"Exemplo do primeiro chunk: {chunks[0][:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0d5ab",
   "metadata": {},
   "source": [
    "## ETAPA 4: Geração de Embeddings\n",
    "\n",
    "**O que são Embeddings?**\n",
    "Representações numéricas (vetores) de textos que capturam seu significado semântico. Textos similares têm vetores próximos no espaço vetorial.\n",
    "\n",
    "**Justificativa Técnica:**\n",
    "O modelo `all-MiniLM-L6-v2` transforma cada chunk em um vetor de **384 números**. Mais dimensões = mais capacidade de capturar nuances. Com apenas 10 dimensões, \"banco\" (assento) e \"banco\" (instituição) ficariam próximos demais. Com 384 dimensões, o modelo separa contextos diferentes.\n",
    "\n",
    "### Matemática: Como Medir Similaridade\n",
    "\n",
    "**Cosine Similarity (Similaridade do Cosseno):**\n",
    "```\n",
    "similarity = (A · B) / (||A|| × ||B||)\n",
    "```\n",
    "\n",
    "Onde:\n",
    "- `A · B` = produto escalar dos vetores\n",
    "- `||A||` = tamanho do vetor A\n",
    "- **Resultado:** -1 (opostos) a +1 (idênticos)\n",
    "\n",
    "**Por que Cosine?**\n",
    "- Mede o **ângulo** entre vetores, não distância absoluta\n",
    "- Textos curtos e longos podem ser comparados de forma justa\n",
    "- Foca na \"direção semântica\"\n",
    "\n",
    "**Exemplo prático:**\n",
    "- \"cachorro\" → [0.2, 0.8, 0.1, ..., 0.3] (384 números)\n",
    "- \"cão\" → [0.19, 0.81, 0.09, ..., 0.29] → similarity ≈ **0.95** (muito similar!)\n",
    "- \"computador\" → [0.7, 0.1, 0.9, ..., 0.6] → similarity ≈ **0.12** (bem diferente)\n",
    "\n",
    "**Por que usar embeddings?**\n",
    "- Busca por **significado**, não apenas palavras exatas\n",
    "- \"Como treinar modelos?\" encontra \"treinamento de algoritmos\"\n",
    "- Muito mais poderoso que busca tradicional por keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "19006aff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Gerados 14 embeddings de 384 dimensões\n",
      "Exemplo (primeiros 10 valores): [ 0.1015088  -0.03895828 -0.02834271 -0.04518022 -0.02155408 -0.02779558\n",
      " -0.00316708  0.07924382 -0.025738    0.03808332]\n"
     ]
    }
   ],
   "source": [
    "# PASSO 4: GERAÇÃO DE EMBEDDINGS (Conversão de texto em vetores numéricos)\n",
    "# Carregar modelo que converte texto em vetores para busca semântica\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Converter cada chunk em um vetor numérico (embedding)\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "print(f\"✓ Gerados {len(embeddings)} embeddings de {len(embeddings[0])} dimensões\")\n",
    "print(f\"Exemplo (primeiros 10 valores): {embeddings[0][:10]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b9c68111",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando IDs automaticamente\n",
    "uids = [f\"doc_{i}\" for i in range(len(chunks))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630901f0",
   "metadata": {},
   "source": [
    "## ETAPA 5: Armazenamento no VectorDB (ChromaDB)\n",
    "\n",
    "**O que faz o ChromaDB?**\n",
    "Armazena os embeddings de forma otimizada para busca rápida por similaridade.\n",
    "\n",
    "**Justificativa Técnica:**\n",
    "O VectorDB não é apenas um \"banco de dados normal\". Ele usa estruturas de dados especializadas (como HNSW - Hierarchical Navigable Small World) para encontrar vetores similares em tempo **logarítmico** O(log n), em vez de comparar com todos os vetores O(n). Isso permite buscar entre milhões de documentos em milissegundos.\n",
    "\n",
    "**Estrutura dos dados:**\n",
    "- `documents`: Textos originais dos chunks (para retornar ao usuário)\n",
    "- `embeddings`: Vetores numéricos (384 dimensões cada) - usados na busca\n",
    "- `ids`: Identificadores únicos (doc_0, doc_1, doc_2, ...) - para rastreamento\n",
    "\n",
    "**Como funciona a busca?**\n",
    "1. Você faz uma pergunta: \"Como funciona machine learning?\"\n",
    "2. A pergunta é convertida em embedding (mesmo modelo: all-MiniLM-L6-v2)\n",
    "3. ChromaDB calcula a **cosine similarity** entre o embedding da pergunta e todos os chunks\n",
    "4. Retorna os chunks mais próximos (mais similares) - geralmente top 3 a 5\n",
    "\n",
    "**Métrica de distância:**\n",
    "Usa distância cosseno - quanto menor, mais similar:\n",
    "\n",
    "- 0.0 = idênticos (cosine similarity = 1.0) **CUIDADO:** ChromaDB retorna **distance** (distância), não similarity. Por isso: menor = melhor!\n",
    "\n",
    "- 0.5 = moderadamente similares (cosine similarity = 0.5)\n",
    "- 1.0 = completamente diferentes (cosine similarity = 0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "5170dc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Banco vetorial criado/atualizado com 14 documentos\n"
     ]
    }
   ],
   "source": [
    "# PASSO 5: CRIAÇÃO DO BANCO DE DADOS VETORIAL (VectorDB)\n",
    "# Criar banco de dados vetorial para armazenar e buscar embeddings\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Nome da collection\n",
    "col_name = \"machlrn\"\n",
    "\n",
    "# Se já existir, obter a collection; caso contrário, criar\n",
    "existing_collections = [c.name for c in client.list_collections()]\n",
    "if col_name in existing_collections:\n",
    "    collection = client.get_collection(name=col_name)\n",
    "else:\n",
    "    collection = client.create_collection(name=col_name)\n",
    "\n",
    "# Adicionar (ou atualizar) chunks, embeddings e IDs ao banco de dados\n",
    "# Usar upsert se disponível para evitar erro com ids já existentes\n",
    "if hasattr(collection, \"upsert\"):\n",
    "    collection.upsert(\n",
    "        documents=chunks,      # Textos originais\n",
    "        embeddings=embeddings, # Vetores numéricos\n",
    "        ids=uids               # Identificadores únicos\n",
    "    )\n",
    "else:\n",
    "    # Fallback para add (pode falhar se ids já existirem)\n",
    "    collection.add(\n",
    "        documents=chunks,      # Textos originais\n",
    "        embeddings=embeddings, # Vetores numéricos\n",
    "        ids=uids               # Identificadores únicos\n",
    "    )\n",
    "\n",
    "print(f\"✓ Banco vetorial criado/atualizado com {len(chunks)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "c2198d96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ids': [['doc_0']], 'embeddings': None, 'documents': [['busco informar diferentemente busca exaustivo onde saber bom nó fronteira expander busca heurístico estimo bom nó fronteira expander base fune']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None]], 'distances': [[0.8819772005081177]]}\n",
      "{'ids': [['doc_6']], 'embeddings': None, 'documents': [['através funo heurístico estratégia sofrer mesmo problema busca exaustivo cega profundidade fazer completo pois poder ocorrer loop exemplo ótimo']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None]], 'distances': [[0.774673581123352]]}\n"
     ]
    }
   ],
   "source": [
    "# Realizar a busca usando collection.query\n",
    "query_embedding_1 = model.encode([\"Quais tipos de busca informada existem?\"])\n",
    "query_embedding_2 = model.encode([\"Quais os problemas da busca exaustiva?\"])\n",
    "\n",
    "results_1 = collection.query(query_embeddings=query_embedding_1, n_results=1)\n",
    "print(results_1)\n",
    "\n",
    "results_2 = collection.query(query_embeddings=query_embedding_2, n_results=1)\n",
    "print(results_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e103e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Resultados da Query 1: Quais tipos de busca informada existem? ===\n",
      "\n",
      "ID: doc_0\n",
      "Distância: 0.8819772005081177\n",
      "Documento: busco informar diferentemente busca exaustivo onde saber bom nó fronteira expander busca heurístico estimo bom nó fronteira expander base fune\n",
      "----------------------------------------\n",
      "\n",
      "=== Resultados da Query 2: Quais os problemas da busca exaustiva? ===\n",
      "\n",
      "ID: doc_6\n",
      "Distância: 0.774673581123352\n",
      "Documento: através funo heurístico estratégia sofrer mesmo problema busca exaustivo cega profundidade fazer completo pois poder ocorrer loop exemplo ótimo\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Imprimir os resultados da primeira query\n",
    "print(\"=== Resultados da Query 1: Quais tipos de busca informada existem? ===\\n\")\n",
    "for i in range(len(results_1['ids'][0])):\n",
    "    doc_id = results_1['ids'][0][i]\n",
    "    distance = results_1['distances'][0][i]\n",
    "    document = results_1['documents'][0][i]\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Distância: {distance}\")\n",
    "    print(f\"Documento: {document}\")\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "print(\"\\n=== Resultados da Query 2: Quais os problemas da busca exaustiva? ===\\n\")\n",
    "for i in range(len(results_2['ids'][0])):\n",
    "    doc_id = results_2['ids'][0][i]\n",
    "    distance = results_2['distances'][0][i]\n",
    "    document = results_2['documents'][0][i]\n",
    "    print(f\"ID: {doc_id}\")\n",
    "    print(f\"Distância: {distance}\")\n",
    "    print(f\"Documento: {document}\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a525f1",
   "metadata": {},
   "source": [
    "### Avaliação:\n",
    "\n",
    "- **Query 1:** Distância de 0.88 - boa acurácia\n",
    "- **Query 2:** Distância de 0.77 - excelente acurácia\n",
    "\n",
    "Ambas as queries retornaram trechos relevantes do PDF que responderiam corretamente as perguntas.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
